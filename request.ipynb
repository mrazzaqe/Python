{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(\"picture\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'd:\\\\MLDS with Python\\\\Python\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "requests.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send the get request\n",
    "url =\"https://www.techgeekbuzz.com/\"\n",
    "source = requests.get(url)\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "html_page = BeautifulSoup(source.text,'html.parser' )\n",
    "images = html_page.find_all(\"img\")\n",
    "\n",
    "for index, image in enumerate(images):\n",
    "    #img src value\n",
    "    image_url = image.get(\"src\")\n",
    "    #print(image_url)\n",
    "    #get image extension\n",
    "    image_extension= image_url.split(\".\")[-1]\n",
    "    #print(image_extension)\n",
    "    #get the image data\n",
    "    image_byte = requests.get(image_url).content\n",
    "    #print(image_byte)\n",
    "    if image_byte:\n",
    "        with open(f\"Image {index+1}.{image_extension}\",\"wb\") as file:\n",
    "            file.write(image_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.rmdir(\"picture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder\n",
    "os.mkdir(\"Picture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'd:\\\\MLDS with Python\\\\Python\\\\Picture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://gonouniversity.edu.bd/pharmacy/gallery/\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = BeautifulSoup(response.text, 'html.parser')\n",
    "images = html_page.find_all(\"img\")\n",
    "\n",
    "for index, image in enumerate(images):\n",
    "    image_url =image.get(\"src\")\n",
    "    image_extension = image_url.split(\".\")[-1]\n",
    "    image_byte =requests.get(image_url).content\n",
    "    if image_byte:\n",
    "        with open(f\"Image {index+1}.{image_extension}\", \"wb\") as file:\n",
    "            file.write(image_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'd:\\\\MLDS with Python\\\\Python\\\\Picture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://www.techgeekbuzz.com/\"\n",
    "req =requests.get(url)\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = BeautifulSoup(req.text,'html.parser')\n",
    "all_images=page.find_all(\"img\")\n",
    "for index, image in enumerate(all_images):\n",
    "    image_url=image.get(\"src\")\n",
    "    image_extension=image_url.split(\".\")[-1]\n",
    "    image_byte=requests.get(image_url).content\n",
    "    if image_byte:\n",
    "        with open(f\"Image{index+1}.{image_extension}\",\"wb\") as file:\n",
    "            file.write(image_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
    "resp = requests.get(url=url)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text\n",
    "resp.cookies\n",
    "resp.json\n",
    "resp.url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('https://reqres.in/api/users')\n",
    "resp\n",
    "resp.status_code, resp.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = resp.json()\n",
    "data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,3):\n",
    "    resp = requests.get('https://reqres.in/api/users', params={'page':page})\n",
    "    data=[]\n",
    "    for user in resp.json()['data']:\n",
    "        user_id = user['id']\n",
    "        data.append(user_id)\n",
    "        f_name = user['first_name']\n",
    "        data.append(f_name)\n",
    "        l_name = user['last_name']\n",
    "        data.append(l_name)\n",
    "        email = user['email']\n",
    "        data.append(email)\n",
    "\n",
    "data        \n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for page in range(1,3):\n",
    "    resp = requests.get('https://reqres.in/api/users', params={'page':page})\n",
    "    for user in resp.json()['data']:\n",
    "        data = resp.json()\n",
    "        print(data)\n",
    "        x =json.dumps(data)\n",
    "        #print(x)\n",
    "        #y =json.loads(x)\n",
    "        #print(y) \n",
    "        #type(y)\n",
    "       # with open(\"test.json\", mode=\"w\") as file:\n",
    "            #writer = file.write(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =r\"https://quotes.toscrape.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ABDURR~1\\AppData\\Local\\Temp/ipykernel_7208/3463565994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tag-item\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ref'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python3.10\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2252\u001b[0m         \u001b[1;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2253\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m   2254\u001b[0m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2255\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "source = requests.get(url)\n",
    "soup =BeautifulSoup(source.content, 'lxml')\n",
    "ar= soup.find_all('span', class_=\"tag-item\")\n",
    "ar.find_all('a', class_='ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(soup):\n",
    "    quotes = soup.find_all('div', attrs={\"class\": \"quote\"})\n",
    "    Quotes = [[quote.span.string,\n",
    "               quote.find('small', class_=\"author\").string,\n",
    "              '#'+'#'.join([tag.string for tag in quote.div.find_all('a', class_='tag')])]\n",
    "              for quote in quotes]\n",
    "    return Quotes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = requests.get(url)\n",
    "soup =BeautifulSoup(source.content, 'lxml')\n",
    "soup.find('li', class_='next').a.attrs['href'].split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://quotes.toscrape.com/tag/love/page/1/\n",
      "https://quotes.toscrape.com/tag/love/page/2/\n",
      "Scraping Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "database = []\n",
    "url = r'https://quotes.toscrape.com/tag/love/page/1/'\n",
    "while True:\n",
    "    try:\n",
    "        source = requests.get(url)\n",
    "        soup = BeautifulSoup(source.content, 'lxml')\n",
    "        quotes = extract(soup)\n",
    "        database = database + quotes\n",
    "        print(url)\n",
    "        page = soup.find('li', class_='next').a.attrs['href'].split('/')[-2]\n",
    "        url = f'https://quotes.toscrape.com/tag/love/page/{page}/'\n",
    "    except AttributeError as e:\n",
    "        print(\"Scraping Completed Successfully!\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "955b5dbf1c9568fcdca6427b82ed9dd3b5652756bab72d537c2112a7dd2607f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
